# ğŸ§  NEURA 
**Chat to Action - Multi task Execution System**  
> *Neura is more than just a chat â€” itâ€™s a real command executor.*
---

## ğŸ“Œ Overview  
Neura is a locally running, multi-intent natural language command execution system. Instead of writing individual commands, you simply chat in plain English â€” and Neura intelligently understands, splits, classifies, and executes each tasks.

It can:

- ğŸ“§ Send emails  
- ğŸµ Play music  
- ğŸ“… Schedule events  
- ğŸŒ¦ï¸ Fetch weather  
- ğŸ“° Deliver news  
â€”all in one go.

---

## ğŸ¯ Key Highlights

- âœ… Multi-intent message handling  
- âš™ï¸ Modular handler-based design  
- âš™ï¸ Finetuning of pretrained model T5 for query splitting, and use of other pretrained model and APIs for rest of the tasks
- ğŸ’¡ LLM-based email generation (local Mistral via ollama)  
- ğŸŒ React frontend + FastAPI backend  

---

## ğŸ§± Project Structure

```bash
GENAI_ASSISTANT/
â”‚
â”œâ”€â”€ app/                        
â”‚   â”œâ”€â”€ config/                 
â”‚   â”‚   â”œâ”€â”€ credentials.json
â”‚   â”‚   â”œâ”€â”€ model_config.json
â”‚   â”‚   â”œâ”€â”€ secrets.json
â”‚   â”‚   â””â”€â”€ token.pickle
â”‚   â”œâ”€â”€ data/                   
â”‚   â”‚   â””â”€â”€ queries.jsonl
â”‚   â”œâ”€â”€ finetuned_query_splitter/  # T5 model files
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ spiece.model
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ handlers/               
â”‚   â”‚   â”œâ”€â”€ calendar.py
â”‚   â”‚   â”œâ”€â”€ client_secret.json
â”‚   â”‚   â”œâ”€â”€ email_handler.py
â”‚   â”‚   â”œâ”€â”€ general.py
â”‚   â”‚   â”œâ”€â”€ local_llm_generator.py
â”‚   â”‚   â”œâ”€â”€ music.py
â”‚   â”‚   â”œâ”€â”€ news.py
â”‚   â”‚   â””â”€â”€ weather.py
â”‚   â”œâ”€â”€ memory_store/
â”‚   â”œâ”€â”€ offload/
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ intent_classifier.py
â”‚   â”œâ”€â”€ langchain_agent.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ query_splitter.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ token.pickle
â”‚
â”œâ”€â”€ genai-ui/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatBubble.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MicInput.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.css
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.jsx
â”‚   â”‚   â”œâ”€â”€ components/  
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ AppRouter.jsx
â”‚   â”‚   â”œâ”€â”€ Sidebar.jsx
â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ package-lock.json
â”‚
â”œâ”€â”€ venv/                       # Python virtual environment
â”œâ”€â”€ db.sqlite3                 
â”œâ”€â”€ .env                        
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ .gitignore
â””â”€â”€ token.pickle
```

---

## âš™ï¸ Core Workflow

### Step 1: Message Input  
User types or speaks a message:
```text
"Email my boss at name@gmail.com about the meeting, set a reminder for meeting tomorrow, and play the song Viva la Vida."
```

### Step 2: Query Splitting  
The message is split using a fine-tuned T5-small model (1000 samples), with fallback logic to preserve tokens (like emails, names).

### ğŸ” Fine-tuned Query Splitter Model

This project includes a custom fine-tuned **T5-small** model for intelligently splitting complex, multi-intent user queries into structured sub-commands. This enables the assistant to process and route multiple tasks from a single user input, such as:

---

#### ğŸ”§ Fine-tuning Details

- **Base model:** [`t5-small`](https://huggingface.co/t5-small)
- **Training platform:** Google Colab
- **Library stack:** `transformers`, `datasets`, `sentencepiece`, `torch`
- **Training dataset:** `queries.jsonl`  
- **Dataset Size:** `1064 samples`
- A JSONL file containing paired examples of `input` (user command) and `output` (query split).
- **Epochs:** 3
- **Loss values:**
  - Epoch 1: `0.1466`
  - Epoch 2: `0.0144`
  - Epoch 3: `0.0102`

---

#### ğŸ“ Model Artifacts

After fine-tuning, model files were exported and stored in finetuned_query_splitter. These are loaded at runtime to perform context-aware query segmentation.

```
app/finetuned_query_splitter/
â”œâ”€â”€ config.json
â”œâ”€â”€ model.safetensors
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ spiece.model
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ tokenizer_config.json
```

---


### Step 3: Intent Classification  
Each split is classified via:

ğŸ” A DistilBERT model  
â• A keyword-based rule system  

### Step 4: Intent Routing  
Each intent is mapped to a function in the `handlers/` folder, for example:  

- `email` â†’ `email_handler.py`  
- `music` â†’ `music.py`  

### Step 5: Execution + Response  
Each task is executed:

- âœ‰ï¸ Emails via Gmail API + LLM generation (mistral via Ollama) 
- â˜ï¸ Weather from OpenWeather API  
- ğŸ—“ï¸ Calendar via Google Calendar API  
- ğŸ“° News from NewsAPI  
- ğŸ§ Music via YouTube Music API  

The response is returned and rendered in the chat UI.

---

## ğŸ§ª Features

| Feature                  | Description                                |
|--------------------------|--------------------------------------------|
| ğŸ” Multi-intent input     | Handle many actions in one query           |
| ğŸ§  Finetuned Query Splitter | finetuning of on T5-small on local dataset of 1064 samples  |
| ğŸ§­ Hybrid Intent Classifier | BERT + rules                              |
| ğŸ’Œ Local LLM Generator    | Mistral (Ollama) for text generation       |
| ğŸ–¥ï¸ Chat UI                | React + Vite                               |
| ğŸ™ï¸ Voice Input            | Web Speech API                             |
| ğŸ’¬ Chat Memory & Export   | Save, rename, export chats                 |

---

## ğŸ’» Setup & Installation

### ğŸ”§ Backend (FastAPI)

```bash
cd GENAI_ASSISTANT/app
python -m venv venv
source venv\Scripts\activate
pip install -r ../requirements.txt
uvicorn app.api:app --reload --port (portnumber)
```

Make sure to:

- Add your Gmail OAuth credentials in `config/client_secret.json`
- Set up `.env` for environment variables
- Run Ollama and load mistral model locally

### ğŸ§  Local LLM Setup (Mistral via Ollama)

```bash
Install Ollama from: https://ollama.com/download
```

```bash
ollama pull mistral
ollama run mistral
```

### ğŸ–¼ï¸ Frontend (React)

```bash
cd genai-assistant/genai-ui
npm install
npm run dev
```

---

## ğŸ§ª Sample Input & Output (to be updated soon)

**Input:**
> 

**Neuraâ€™s Actions:**

- 
- 
- 

---

## ğŸ§  Model Details

| Component          | Model                                         |
|--------------------|-----------------------------------------------|
| Query Splitter     | `t5-small` (finetuned)                        |
| Intent Classifier  | `distilbert-base-uncased-finetuned-sst-2-english` |
| Text Generation    | `mistral` (local via Ollama)                  |

---

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI, Transformers, Pydantic, Google API, Mistral (Ollama)  
- **Frontend**: React, Vite, Tailwind CSS, Web Speech API  
- **LLM**: Local model (no OpenAI key)  
- **APIs**: Gmail API, Google Calendar API, YouTube Music, OpenWeather, NewsAPI  

---

## ğŸ“‚ Future Improvements

- â³ Add database persistence for reminders  
- ğŸ—£ï¸ Support multi-language commands  
- ğŸ§© More plugin-style handlers (e.g., notes, file upload)  

---

## ğŸ“œ License  
This project is open-source and free to use for educational or non-commercial use.